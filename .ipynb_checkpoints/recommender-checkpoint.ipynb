{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System\n",
    "I will now build a basic recommender system based for the books that have been scraped.  The idea is that you give it a single book and it will return books you are likely to also enjoy based on their similarity to the book that you provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type\n",
    "While there are many types of recommender systems, the two most common are *collaborative filters* and *content filters*.\n",
    "\n",
    "At a high level, collaborative filtering works at a user-level.  It takes individual statistics like ratings, which items were viewed, etc., and draws similarities between users based on these values.  If there is content that one has interacted with that another did not, it can be a potential suggestion.\n",
    "\n",
    "On the other hand, content filters ignore the user and focus on the similarities between the actual content of the data, such as weighted ratings, similarity of authors, frequency of topics appearing in the description, and so on.  This method requires a direct \n",
    "'similarity score' between items in order to compute how related they are.\n",
    "\n",
    "I'm going to go with the **content filtering** method because the data that I scraped best fits this - it has book content, not user interaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rake_nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b0cde182169e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# NLP stuff.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrake_nltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRake\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordpunct_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rake_nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP stuff.\n",
    "import string\n",
    "from rake_nltk import Rake\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = pd.read_csv('./scraper/output/pages-1-100.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates\n",
    "I have read on the user forum and eyeballed a few duplicates.  I will remove them by common title.  Of course the disadvantage to this is that some removes entries may contain information that's missing in the first encounter (which is what is kept by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data.drop_duplicates(subset='title', inplace=True)\n",
    "\n",
    "# Resetting the index is VERY important!\n",
    "# We rely on the index later and if we remove values here, the index will no longer be right.\n",
    "book_data = book_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted rating & top books\n",
    "We cannot take rating scores directly as they can be imbalanced.  One user rating a book 5/5 is not better than 50,000 people rating it on average 4.5.  We need some kind of algorithm to weight the rating values.\n",
    "\n",
    "[IMDB's FAQ](https://help.imdb.com/article/imdb/track-movies-tv/ratings-faq/G67Y87TFYYP6TWAV?ref_=helpms_helpart_inline#calculatetop) describes the algorithm that they use to weight the rank o movies and TV shows for the top rated lists.  It reads:\n",
    "\n",
    "$\\text{Weighted Rating (WR)} = (\\frac{v}{v+m} \\cdot R) + (\\frac{m}{v+m} \\cdot C)$\n",
    "\n",
    "where\n",
    "\n",
    "* $R$ is the average rating for the movie (mean).\n",
    "* $v$ is the number of votes for the movie.\n",
    "* $m$ is the minimum number of votes to be listed (25,000 in their case)\n",
    "* $C$ is the mean vote across the whole report.\n",
    "\n",
    "We already have access to $R$ and $v$ in the columns directly.  $C$ is something we can compute from the data.  $m$ is something we can configure and tweak.  I'll begin with the 10th percentile, essentially chopping off the bottom part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = book_data['avg_rating'].mean()\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = book_data['num_ratings'].quantile(0.1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rating(book, m, C):\n",
    "    # Average rating for the book.\n",
    "    R = book['avg_rating']\n",
    "    # Total number of votes for the book.\n",
    "    v = book['num_ratings']\n",
    "    # IMDB formula.\n",
    "    return (v / (v+m) * R) + (m / (m+v) * C)\n",
    "\n",
    "# Calculate the weighted rating for books that are within our threshold.\n",
    "book_data.loc[book_data.num_ratings > m, 'weighted_rating'] = book_data.loc[book_data.num_ratings > m].apply(lambda x: weighted_rating(x, m, C), axis=1)\n",
    "\n",
    "# Fill the NaN values (i.e., books lower than our threshold) with a zero score.\n",
    "book_data['weighted_rating'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, let's eyeball the top and bottom 5 entries (sorted by `weighted_rating`).  These movies are 'similar' in that they are ordered by their weighted rating.  Books around the same score were rated similar.  However, this is too simple and doesn't consider what the actual books are about, who wrote them, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data.sort_values('weighted_rating', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data.sort_values('weighted_rating', ascending=False).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little cleanup.\n",
    "del C\n",
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Recommender System\n",
    "Now let's get to building the recommender.  It will be based on the content, so we will be creating an amalgam of features per book that will be used to calculate the similarity score between books.\n",
    "\n",
    "Values I'm thinking of using include the title, series that it belongs to (if any), language, author(s), genres, and of course we can identify keywords from the book's description.\n",
    "\n",
    "Instead of treating each entry equally, we can add weight to them by mentioning the words multiple times in the vector that we will use to calculate similarity.\n",
    "\n",
    "Problems with the approach I have taken below include:\n",
    "\n",
    "* Genres and languages can overlap (English vs. English) which increases the importance of that feature.\n",
    "* Processing is a little trivial without much testing yet.\n",
    "* All authors are included blindly.  They could be filtered based on their (Role)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a string and returns an array of its processed words.\n",
    "def clean_string(s):\n",
    "    # Remove stopwords and punctuation.\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    return [n for n in wordpunct_tokenize(s.lower()) if n not in stop]\n",
    "\n",
    "def create_soup(x):\n",
    "    title_importance = 1\n",
    "    language_importance = 1\n",
    "    series_importance = 1\n",
    "    authors_importance = 1\n",
    "    genres_importance = 1\n",
    "\n",
    "    soup = ''\n",
    "    \n",
    "    # Keywords from description.\n",
    "    desc = x['description']\n",
    "    if desc is not np.nan:\n",
    "        rake = Rake()\n",
    "        rake.extract_keywords_from_text(desc)\n",
    "        desc_soup = ' '.join(list(rake.get_word_degrees().keys()))\n",
    "        soup = ' '.join(filter(None, [soup, desc_soup]))\n",
    "    \n",
    "    # Title.\n",
    "    title_soup = ' '.join(clean_string(x['title']) * title_importance)\n",
    "    soup = ' '.join(filter(None, [soup, title_soup]))\n",
    "    \n",
    "    # Language.\n",
    "    language = x['language']\n",
    "    if language is not np.nan:\n",
    "        language_soup = ' '.join(clean_string(language) * language_importance)\n",
    "        soup = ' '.join(filter(None, [soup, language_soup]))\n",
    "    \n",
    "    # Series.\n",
    "    series = x['series']\n",
    "    if series is not np.nan:\n",
    "        series_soup = ' '.join(clean_string(series) * series_importance)\n",
    "        soup = ' '.join(filter(None, [soup, series_soup]))\n",
    "\n",
    "    # Authors.\n",
    "    authors = x['authors']\n",
    "    if authors is not np.nan:\n",
    "        # I'm trying to not remove punctuation here but to just set all as spaces. I want to retain (Role).\n",
    "        # Providing it's consistent across entries, this should work.\n",
    "        author_soup = ' '.join([a.lower().replace(' ', '') for a in authors.split(',')] * authors_importance)\n",
    "        soup = ' '.join(filter(None, [soup, author_soup]))\n",
    "    \n",
    "    # Genres.\n",
    "    genres = x['genres']\n",
    "    if genres is not np.nan:\n",
    "        # Almost the same treatment as authors (strip spaces to make matching a bit more likely).\n",
    "        genre_soup = ' '.join([g.lower().replace(' ', '') for g in genres.split(',')] * genres_importance)\n",
    "        soup = ' '.join(filter(None, [soup, genre_soup]))\n",
    "    \n",
    "    return soup\n",
    "\n",
    "book_data['soup'] = book_data.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data.soup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create the similarity matrix between all books based on our lovely steaming soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "count_matrix = count_vec.fit_transform(book_data['soup'])\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cos_sim = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse lookup of title vs. index.\n",
    "title_to_index = pd.Series(book_data.index, index=book_data['title'])\n",
    "\n",
    "def get_recommendation(title):\n",
    "    idx = title_to_index[title]\n",
    "    print(idx)\n",
    "    print(book_data.loc[idx].soup)\n",
    "    \n",
    "    scores = pd.Series(cos_sim[idx]).sort_values(ascending=False)\n",
    "    book_indices = list(scores.iloc[1:11].index)\n",
    "    \n",
    "#     scores = list(enumerate(cos_sim[idx]))\n",
    "#     scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "#     scores = scores[1:11]\n",
    "#     book_indices = [i[0] for i in scores]\n",
    "    print(scores[1:11])\n",
    "    return book_data.iloc[book_indices]\n",
    "\n",
    "# get_recommendation('Harry Potter and the Chamber of Secrets')\n",
    "get_recommendation(\"The Hitchhiker's Guide to the Galaxy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to now output the data to some pickle files for loading elsewhere (since it has been processed a little)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "should_export = False\n",
    "\n",
    "if should_export:\n",
    "    # Book data.\n",
    "    print('Exporting book data...', end='')\n",
    "    pickle.dump(book_data, open('book_data.pickle', 'wb'))\n",
    "    print('done!')\n",
    "    \n",
    "    # Cosine similarity (warning: this will be huge).\n",
    "    print('Exporting similarity matrix...', end='')\n",
    "    pickle.dump(cos_sim, open('cossim.pickle', 'wb'))\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Easy way to find the rows of books I know.\n",
    "book_data.loc[book_data.title.str.contains('Hitchhiker')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
